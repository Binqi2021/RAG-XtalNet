#!/usr/bin/env python3
"""
XtalNet CCSG Results Analysis Script

This script aggregates and analyzes multiple CCSG experiment results for comparison
between baseline XtalNet and RAG-XtalNet variants. It processes metrics files
generated by compute_ccsg_metrics.py and outputs aggregated statistics.

Usage:
    python scripts/analyze_ccsg_results.py \
        --result_dirs exp1/baseline exp2/rag exp3/rag_align \
        --output analysis_results.csv \
        --format csv

    python scripts/analyze_ccsg_results.py \
        --result_dirs exp1/baseline exp2/rag \
        --output analysis_results.json \
        --format json
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import numpy as np
import pandas as pd

def find_metrics_file(result_dir: Path) -> Optional[Path]:
    """Find metrics file in result directory (supports various naming patterns)"""
    patterns = [
        "metrics.json",
        "ccsg_metrics.json",
        "results_metrics.json",
        "evaluation_metrics.json"
    ]

    for pattern in patterns:
        metrics_file = result_dir / pattern
        if metrics_file.exists():
            return metrics_file

    # Search for any .json file that might contain metrics
    json_files = list(result_dir.glob("*.json"))
    for json_file in json_files:
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                if any(key in data for key in ['match_at_k', 'rmse', 'top_k_matches']):
                    return json_file
        except:
            continue

    return None

def load_metrics(result_dir: Path) -> Dict[str, Any]:
    """Load metrics from a result directory"""
    metrics_file = find_metrics_file(result_dir)

    if not metrics_file:
        print(f"Warning: No metrics file found in {result_dir}")
        return {}

    try:
        with open(metrics_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading metrics from {metrics_file}: {e}")
        return {}

def extract_experiment_name(result_dir: Path) -> str:
    """Extract experiment name from directory path"""
    # Try to get the last directory name
    name = result_dir.name

    # If name is generic, try parent directory
    if name in ['results', 'output', 'eval', 'evaluation']:
        name = result_dir.parent.name

    # Clean up common prefixes/suffixes
    name = name.replace('exp_', '').replace('experiment_', '')
    name = name.replace('_results', '').replace('_output', '')

    return name

def compute_match_at_k_stats(metrics: Dict[str, Any]) -> Dict[str, float]:
    """Compute statistics for match@k metrics"""
    if 'match_at_k' not in metrics:
        return {}

    match_at_k = metrics['match_at_k']
    stats = {}

    for k in [1, 3, 5, 10]:
        if f'match@{k}' in match_at_k:
            stats[f'match_{k}'] = float(match_at_k[f'match@{k}'])
        elif str(k) in match_at_k:
            stats[f'match_{k}'] = float(match_at_k[str(k)])

    return stats

def compute_rmse_stats(metrics: Dict[str, Any]) -> Dict[str, float]:
    """Compute RMSE statistics"""
    if 'rmse' not in metrics:
        return {}

    rmse_data = metrics['rmse']
    stats = {}

    if isinstance(rmse_data, dict):
        if 'mean' in rmse_data:
            stats['rmse_mean'] = float(rmse_data['mean'])
        if 'median' in rmse_data:
            stats['rmse_median'] = float(rmse_data['median'])
        if 'std' in rmse_data:
            stats['rmse_std'] = float(rmse_data['std'])
    elif isinstance(rmse_data, (int, float)):
        stats['rmse_mean'] = float(rmse_data)
        stats['rmse_median'] = float(rmse_data)

    return stats

def compute_atom_bucket_stats(metrics: Dict[str, Any]) -> Dict[str, Dict[str, float]]:
    """Compute statistics for atom count buckets"""
    if 'atom_bucket_rmse' not in metrics:
        return {}

    bucket_data = metrics['atom_bucket_rmse']
    bucket_stats = {}

    for bucket, data in bucket_data.items():
        if isinstance(data, dict):
            bucket_stats[f'bucket_{bucket}_mean'] = float(data.get('mean', 0))
            bucket_stats[f'bucket_{bucket}_median'] = float(data.get('median', 0))
        elif isinstance(data, (int, float)):
            bucket_stats[f'bucket_{bucket}_mean'] = float(data)
            bucket_stats[f'bucket_{bucket}_median'] = float(data)

    return bucket_stats

def analyze_single_experiment(result_dir: Path, split: str = 'test') -> Dict[str, Any]:
    """Analyze a single experiment directory"""
    experiment_name = extract_experiment_name(result_dir)
    metrics = load_metrics(result_dir)

    if not metrics:
        return {}

    # Extract split-specific metrics if available
    split_metrics = metrics.get(split, metrics)

    analysis = {
        'exp_name': experiment_name,
        'split': split,
        'result_dir': str(result_dir),
        'num_samples': len(split_metrics.get('rmse_list', [])) if isinstance(split_metrics.get('rmse_list'), list) else 0
    }

    # Compute various statistics
    analysis.update(compute_match_at_k_stats(split_metrics))
    analysis.update(compute_rmse_stats(split_metrics))
    analysis.update(compute_atom_bucket_stats(split_metrics))

    # Add any additional metadata
    if 'model_config' in metrics:
        analysis['model_type'] = metrics['model_config'].get('type', 'unknown')
    if 'retrieval_config' in metrics:
        analysis['use_retrieval'] = True
        analysis['retrieval_top_m'] = metrics['retrieval_config'].get('top_m', 'unknown')
    else:
        analysis['use_retrieval'] = False

    return analysis

def aggregate_results(result_dirs: List[Path], splits: List[str] = ['test', 'val']) -> List[Dict[str, Any]]:
    """Aggregate results from multiple experiments"""
    all_results = []

    for result_dir in result_dirs:
        if not result_dir.exists():
            print(f"Warning: Result directory {result_dir} does not exist")
            continue

        for split in splits:
            result = analyze_single_experiment(result_dir, split)
            if result:
                all_results.append(result)

    return all_results

def save_results_csv(results: List[Dict[str, Any]], output_path: Path):
    """Save results to CSV format"""
    # Get all unique keys from all results
    all_keys = set()
    for result in results:
        all_keys.update(result.keys())

    # Convert to DataFrame and save
    df = pd.DataFrame(results)

    # Reorder columns for better readability
    priority_cols = ['exp_name', 'split', 'num_samples', 'use_retrieval', 'model_type']
    other_cols = sorted([col for col in df.columns if col not in priority_cols])
    df = df[priority_cols + other_cols]

    df.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")

def save_results_json(results: List[Dict[str, Any]], output_path: Path):
    """Save results to JSON format"""
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    print(f"Results saved to {output_path}")

def print_summary(results: List[Dict[str, Any]]):
    """Print a summary of the analysis"""
    print("\n" + "="*80)
    print("CCSG RESULTS ANALYSIS SUMMARY")
    print("="*80)

    if not results:
        print("No results to analyze!")
        return

    # Group by experiment name
    exp_groups = {}
    for result in results:
        exp_name = result['exp_name']
        if exp_name not in exp_groups:
            exp_groups[exp_name] = []
        exp_groups[exp_name].append(result)

    print(f"\nAnalyzed {len(results)} results from {len(exp_groups)} experiments:")
    print("-" * 60)

    for exp_name, exp_results in exp_groups.items():
        print(f"\nExperiment: {exp_name}")
        for result in exp_results:
            split = result['split']
            samples = result.get('num_samples', 0)
            use_rag = result.get('use_retrieval', False)
            rag_str = "RAG" if use_rag else "Baseline"

            print(f"  {split.upper()}: {samples} samples ({rag_str})")

            # Print key metrics if available
            if 'match_1' in result:
                print(f"    Match@1: {result['match_1']:.3f}")
            if 'match_5' in result:
                print(f"    Match@5: {result['match_5']:.3f}")
            if 'rmse_mean' in result:
                print(f"    RMSE (mean): {result['rmse_mean']:.4f}")

def main():
    parser = argparse.ArgumentParser(
        description="Analyze CCSG experiment results for baseline vs RAG comparison"
    )
    parser.add_argument(
        "--result_dirs",
        nargs="+",
        required=True,
        help="List of experiment result directories to analyze"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Output file path (CSV or JSON)"
    )
    parser.add_argument(
        "--format",
        choices=["csv", "json"],
        default="csv",
        help="Output format (default: csv)"
    )
    parser.add_argument(
        "--splits",
        nargs="+",
        default=["test", "val"],
        help="Data splits to analyze (default: test val)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print detailed analysis information"
    )

    args = parser.parse_args()

    # Convert to Path objects
    result_dirs = [Path(d) for d in args.result_dirs]
    output_path = Path(args.output)

    # Ensure output directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Aggregate results
    print(f"Analyzing {len(result_dirs)} result directories...")
    results = aggregate_results(result_dirs, args.splits)

    if not results:
        print("Error: No valid results found to analyze!")
        sys.exit(1)

    # Save results
    if args.format == "csv":
        save_results_csv(results, output_path)
    else:
        save_results_json(results, output_path)

    # Print summary
    if args.verbose:
        print_summary(results)

    print(f"\nAnalysis complete! Processed {len(results)} results.")

if __name__ == "__main__":
    main()